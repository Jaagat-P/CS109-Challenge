# -*- coding: utf-8 -*-
"""CS109_FinalProject_JaagatPrashar.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XK51-6XFdgINBzdzXmue1Q_HVVeE96UH
"""

import kagglehub
import pandas as pd
import os

path = kagglehub.dataset_download("kazanova/sentiment140")

files = os.listdir(path)

csv_file = os.path.join(path, 'training.1600000.processed.noemoticon.csv')

# Manually add the column names since they aren't there - based on Kaggle.
column_names = ['sentiment', 'id', 'date', 'query', 'user', 'text']
df = pd.read_csv(csv_file, encoding='latin-1', names=column_names)
df.head()

df.head()

print("Unique sentiment vals:", df['sentiment'].unique())
print("Sentiment value count:", df['sentiment'].value_counts())
# There are only two sentiment values, where 0 is negative and 4 is positive. The Kaggle dataset is balanced with a 50/50 split.

#Import libraries for experimentation/bayesian analysis
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc
from sklearn.pipeline import Pipeline
from scipy.stats import entropy
import re
from collections import Counter
import math
import warnings
warnings.filterwarnings('ignore')

#Preprocessing function - remove URLs, mentions, hashtags, punctuation, etc.

def preprocess_text(text):
  # First lowercase all text to ensure that we can proceed from a baseline
  text = text.lower()
  text = re.sub(r'http\S+', '', text)
  text = re.sub(r'@\w+', '', text)
  text = re.sub(r'#\w+', '', text)
  text = re.sub(r'[^\w\s]', '', text)
  return text

df['processed_text'] = df['text'].apply(preprocess_text)

df.head()

#Perform feature engineering. Experimenting with shannon entropy (i.e., entropy of each given text). We will apply this on the processed texts generated above!

def calculate_shannon_entropy(text):
  # If no text, don't do anything
  if not text or len(text) <= 1:
    return 0

  # Frequency of each character
  freq = Counter(text)

  # Probability of each character occurring
  length = len(text)
  probabilities = [count/length for count in freq.values()]

  # Compute entropy!
  return entropy(probabilities, base=2)

df['text_entropy'] = df['processed_text'].apply(calculate_shannon_entropy)

df.head(50)

# Create a reference frame or lexicon of concerning terms
# Terms below were primarily from literature review and past posts observed online (with hate speech, school shooters, etc. )
concerning_terms = [
    # Violence-related
    'kill', 'murder', 'shoot', 'gun', 'bomb', 'attack', 'destroy', 'hurt', 'hate',
    # School-related. Not the most reliable, but still important in context.
    'school', 'classroom', 'teacher', 'student', 'campus',
    # Weapons
    'weapon', 'knife', 'rifle', 'bullet', 'ammo', 'firearm', 'pistol',
    # Indication of something being planned
    'plan', 'tomorrow', 'soon', 'going to', 'will', 'revenge', 'payback',
    # Emotional distress
    'suicide', 'die', 'death', 'pain', 'suffer', 'end it all', 'no hope',
    # Threats
    'threat', 'warning', 'watch out', 'be ready', 'deserve'
]

#Count the number of terms in each processed text (this will be for concerning terms, absolutist terms, etc. - a more general function)
def count_term(text, term_list):
  text_lower = text.lower()
  words = text_lower.split()
  count = sum(1 for term in term_list if term in text_lower)
  return count

df['concerning_term_count'] = df['processed_text'].apply(lambda x: count_term(x, concerning_terms))

#Now, in our dataframe above, we have added a new column that directly tells us the number of concerning terms for each processed text.

df.head(50)

#Measure absolutist/extreme language that is often prevalent in these negative/dangerous/alarming posts that need to be taken into consideration.
# I.e. "NOBODY understands me," or "I am ALWAYS upset"

absolutist_terms = ['always', 'never', 'completely', 'totally', 'absolutely', 'everyone','nobody', 'nothing', 'everything', 'all', 'none', 'every', 'any', 'must', 'perfect', 'impossible', 'definitely', 'forever', 'ever']

#Count the nunber of absolutist terms in each processed piece of text. Add a separate column.
df['absolutist_count'] = df['processed_text'].apply(lambda x: count_term(x, absolutist_terms))

df.head(50)

# After careful research, I found that first-person fixation can also play a huge role in these alarming tweets ("I am..., I will..., etc.")
# Again, this may not be the most reliabe medium as we need to consider context entirely. We will explore this as a part of exploratory data analysis.
first_person_terms = ['i', 'me', 'my', 'mine', 'myself']
df['first_person_count'] = df['processed_text'].apply(lambda x: count_term(x, first_person_terms))

df.head(10)

#We can also conduct a temporal analysis for data/time
# Add date, hour, and day of week coluns into df!
df['date'] = pd.to_datetime(df['date'], errors='coerce')
df['hour'] = df['date'].dt.hour
df['day_of_week'] = df['date'].dt.dayofweek

#Studies have shown that more emotional texts tend to be longer. Although we've already considered entropy, we can still add this for data analysis/exporation sake.
df['text_length'] = df['processed_text'].apply(len)
# Use split function to split the sentence/processed text and then len() to count how many words.
df['word_count'] = df['processed_text'].apply(lambda x: len(x.split()))

df.head(10)

# We can establish a treshold for what constitutes a concerning text based upon the concerning term count's 95th percentile.
# We will only take values that are above this 95th percentile (note that the majority of these counts are just 1 so having at least 1 concerning word flags it as concerning)
threshold = df['concerning_term_count'].quantile(0.95)
df['concerning'] = (df['concerning_term_count'] >= threshold).astype(int)



df.head(10)

# We will be performing Naive Bayes below. Gather prior:
prior_concerning = df['concerning'].mean()

prior_concerning

#Naive Bayes - P(concerning|features) is harder to answer than P(features|concerning). Thus, we can use Naive Bayes for every single row considering the features of entropy, concerning term count, and absolutist term counts.

# Note that entropy is continuous so we need to bin/discretize it to calculate probabilities from it and interpret it.

# The function below calculaes the conditional probability given the data frame, feature, bin_edges, and the target value (or ground truth if you will)
def calculate_conditional_probability(df, feature, bin_edges, target = 'concerning'):

  # Discretize continuous feature such as entropy into bins
  df[f'{feature}_bin'] = pd.cut(df[feature], bins=bin_edges, labels=False)

  # Dictonary to store conditional probabilities for each bin for each feature
  conditional_probabilities = {}

  # Consider all concerning cases
  concerning_cases = df[df[target] == 1]
  # P(feature is in a given bin|target = 1) = ?
  if len(concerning_cases) > 0:
    concerning_counts = concerning_cases[f'{feature}_bin'].value_counts(normalize=True).to_dict()
    # Note that we use 0.001 for Laplace smoothing. We want to avoid any bin being 0 as this can lead to complications later down the line.
    # Get proportion of concerning terms for each given bin that we have.
    conditional_probabilities[1] = {bin_idx: concerning_counts.get(bin_idx, 0.01) for bin_idx in range(len(bin_edges)-1)} #n+1 edges, n bins.
  else:
    conditional_probabilities[1] = {bin_idx: 0.01 for bin_idx in range(len(bin_edges)-1)}

  #P(feature is in a given bin|target = 0) = ?
  unconcerning_cases = df[df[target] == 0]
  if len(unconcerning_cases) > 0:
    # Similarly, get proportion of each bin for the unconcerning cases here.
    unconcerning_counts = unconcerning_cases[f'{feature}_bin'].value_counts(normalize=True).to_dict()
    conditional_probabilities[0] = {bin_idx: unconcerning_counts.get(bin_idx, 0.01) for bin_idx in range(len(bin_edges)-1)}
  else:
    conditional_probabilities[0] = {bin_idx: 0.01 for bin_idx in range(len(bin_edges)-1)}

  return conditional_probabilities

# Calculate conditional probabilities for each feature - that is, entropy, concerning term count, and absolutist term count
entropy_bins = np.linspace(df['text_entropy'].min(), df['text_entropy'].max(), 6) #Discretize entropy into 6 bins
concerningterm_bins = range(0, df['concerning_term_count'].max() + 1) #+1 to include max value in our binning
absolutist_bins = range(0, df['absolutist_count'].max() + 1)

# Calculate conditional probabilities. Keys are bin indices and values are proportions (given 0 and 1 as the main keys of not concerning and concerning)
entropy_cond_probs = calculate_conditional_probability(df, 'text_entropy', entropy_bins)
term_cond_probs = calculate_conditional_probability(df, 'concerning_term_count', concerningterm_bins)
absolutist_cond_probs = calculate_conditional_probability(df, 'absolutist_count', absolutist_bins)

print(entropy_cond_probs) # Recall bins (i.e., multiple different instances)

print(term_cond_probs)

print(absolutist_cond_probs)

def predict_risk_bayesian(entropy, concerning_terms, absolutist_count):
    # Find the bins!
    entropy_bin = np.digitize(entropy, entropy_bins) - 1 # -1 to adjust for zero-based indexing!
    term_bin = min(concerning_terms, len(concerningterm_bins) - 2) # minus 2, because bins are defined by edges and we want to reference them by index
    absolutist_bin = min(absolutist_count, len(absolutist_bins) - 2)

    # Limit bin index to valid range.
    entropy_bin = min(max(entropy_bin, 0), len(entropy_bins) - 2)

    # For concerning = 1
    likelihood_pos = (entropy_cond_probs[1].get(entropy_bin, 0.01)*term_cond_probs[1].get(term_bin, 0.01)*absolutist_cond_probs[1].get(absolutist_bin, 0.01))
        # if bin isn't found, we use small laplace smoothing of 0.01
    posterior_pos = likelihood_pos * prior_concerning

    likelihood_neg = (entropy_cond_probs[0].get(entroI py_bin, 0.01)*term_cond_probs[0].get(term_bin, 0.01)*absolutist_cond_probs[0].get(absolutist_bin, 0.01))
    posterior_neg = likelihood_neg * (1 - prior_concerning)

    #Normalize; denominator.
    total = posterior_pos + posterior_neg
    # To prevent case of dividing by 0, have a default value of 0.5 (as a so-called standard).
    if total == 0:
        return 0.5

    return posterior_pos / total

#Now that we have our function rto predict bayesian risk based upon which bin our feature falls in and the probability that our feature is in that given bin, we can apply Bayesian prediction:

df['bayesian_risk'] = df.apply(lambda x: predict_risk_bayesian(x['text_entropy'], x['concerning_term_count'], x['absolutist_count']), axis=1)

#Visualize Bayesian risk. This approach sort of weighs the relevance of each feature in deciding our final probability of risk.
df.head(50)

"""Idea: we can bootstrap between positive and negative?"""

# Install relevant libraries for ML! Experiment around.
!pip install pandas numpy scikit-learn imbalanced-learn -q

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from imblearn.over_sampling import SMOTE

#Replace NaN values with 0s for simplicity when training! We don't want to run into complexities. This isn't very ideal, but is a data limitation.

df['text_entropy_bin'] = df['text_entropy_bin'].fillna(0)
df['concerning_term_count_bin'] = df['concerning_term_count_bin'].fillna(0)
df['absolutist_count_bin'] = df['absolutist_count_bin'].fillna(0)

df.head(10)

# One huge limitation is that our data doesn't have which tweet is dangerous and which tweet isn't dangerous. We can use sentiment being 0 (i.e., the tweet being negative), as almost
# a vague proxy variable for the tweet being dangerous.
df['dangerous'] = (df['sentiment'] == 0).astype(int)

#Define our features to consider in our model. Previously, we didn't find very strong correlations for these features alone, but perhaps together we can reach a more comprehensive solution.
features = ['text_entropy', 'concerning_term_count', 'absolutist_count', 'first_person_count','bayesian_risk']

#Dataset is very large, so for sampling purposes we will randomly sample 500000 from 1.6 million (to prevent Colab from crashing?). Maybe it's worth experimenting and seeing which is the best.
df_sample = df.sample(n=500000, random_state=42)

#Split sample data into train and test
train_df, test_df = train_test_split(df_sample, test_size=0.2, random_state=42)

#From research - we can perform step vectorization, which is very popular/relevant in NLP.
# We will limit the vocab to the 500 most frequent words. Use stop words to remove common words that are essentially meaningless like the, a, or, etc.
vectorizer = CountVectorizer(max_features=500, stop_words='english')
train_words = vectorizer.fit_transform(train_df['text']) # Learn the vocabulary in the data -- text column specifically
test_words = vectorizer.transform(test_df['text']) # Convert to matrix; each row represents a documenta and each column represents word in vocab (or out of those 500 features)

X_train = pd.concat([train_df[features].reset_index(drop=True),pd.DataFrame(train_words.toarray(),columns=vectorizer.get_feature_names_out()).reset_index(drop=True)], axis=1)

X_test = pd.concat([test_df[features].reset_index(drop=True),pd.DataFrame(test_words.toarray(),columns=vectorizer.get_feature_names_out()).reset_index(drop=True)], axis=1)

y_train = (train_df['sentiment'] == 0).astype(int).reset_index(drop=True)

y_test = (test_df['sentiment'] == 0).astype(int).reset_index(drop=True)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

train_accuracy = model.score(X_train, y_train)
test_accuracy = accuracy_score(y_test, y_pred)

print(f"Training Accuracy: {train_accuracy}")
print(f"Test Accuracy: {test_accuracy}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

all_features = features + vectorizer.get_feature_names_out().tolist()
importance = model.feature_importances_
indices = np.argsort(importance)[::-1]

print("\nTop 10 most important features:")
for i, idx in enumerate(indices[:10]):
    print(f"{all_features[idx]}: {importance[idx]:.4f}")